<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="ie ie6" lang="en"> <![endif]-->
<!--[if IE 7 ]><html class="ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html lang="en"> <!--<![endif]-->
<head>

  <!-- Basic Page Needs
  ================================================== -->
  <meta charset="utf-8">
  <title>Elliot Greenwood's Personal Page</title>
  <meta name="description" content="Personal Page">
  <meta name="author" content="Elliot Greenwood">


  <!-- Mobile Specific Metas
  ================================================== -->
  <!--<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">-->
  <script src="//use.typekit.net/xvl2gac.js"></script>
  <script>try{Typekit.load();}catch(e){}</script>

  <!-- CSS
  ================================================== -->
  <style>
  #top,a,abbr,acronym,address,applet,article,aside,audio,b,big,blockquote,body,canvas,caption,center,cite,code,dd,del,details,dfn,div,dl,dl dt,dt,em,embed,fieldset,figcaption,figure,footer,form,h1,h2,h3,h4,h5,h6,header,hgroup,html,i,iframe,img,ins,kbd,label,legend,li,mark,menu,nav,object,ol,output,p,pre,q,ruby,s,samp,section,small,span,strike,strong,sub,summary,sup,table,tbody,td,tfoot,th,thead,time,tr,tt,u,ul,var,video{margin:0;padding:0;border:0;font:inherit;vertical-align:baseline}article,aside,details,figcaption,figure,footer,header,hgroup,menu,nav,section{display:block}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:after,blockquote:before,q:after,q:before{content:'';content:none}::selection{color:#050505;background:#c7c7c7}::-moz-selection{color:#050505;background:#c7c7c7}html{color:#272727;font-family:Calluna,Georgia,Times,"Times New Roman",serif;font-variant-ligatures:additional-ligatures;-webkit-font-variant-ligatures:additional-ligatures;text-rendering:optimizeLegibility;font-kerning:normal}p{margin:0 0 18px;font-size:1em;line-height:1.4em;text-align:justify;-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;hyphens:auto}p.lead{font-size:1em;line-height:1.5em}h1{font-size:3em;line-height:1.125em}h2{font-size:2.25em;line-height:1.5em}#top,h3{font-size:1.5em;line-height:2em;font-weight:700}h4{font-size:1em;line-height:1.5em;font-weight:700}h5{line-height:1.5em;font-weight:700}a,a:visited{color:#050505}a:focus,a:hover{text-decoration:none}a:focus.purple,a:hover.purple{color:#cd9aba}a:focus.blue,a:hover.blue{color:#8baecc}a:focus.green,a:hover.green{color:#a4c39c}a:focus.yellow,a:hover.yellow{color:#e3cf56}nav a,nav a:visited{text-decoration:none;color:#050505}.classification header>nav a:focus,.classification header>nav>ul>li>a:hover{color:#881058}.regression header>nav a:focus,.regression header>nav>ul>li>a:hover{color:#0e5f9b}.techniques header>nav a:focus,.techniques header>nav>ul>li>a:hover{color:#277512}.summary header>nav a:focus,.summary header>nav>ul>li>a:hover{color:#dbbd0b}.home header>nav a:focus,.home header>nav>ul>li>a:hover,.listen header>nav a:focus,.listen header>nav>ul>li>a:hover{color:#c7c7c7}em{font-style:italic}dl dt,h1,h2,strong{font-weight:700}code,pre{font-family:prestige-elite-std,Menlo,monospace}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sup{top:-.5em}sub{bottom:-.25em}blockquote{margin:10px 50px 10px 0;padding-left:15px;border-left:3px solid #c7c7c7}.classification blockquote{color:#881058;border-color:#881058}.regression blockquote{color:#0e5f9b;border-color:#0e5f9b}.techniques blockquote{color:#277512;border-color:#277512}.summary blockquote{color:#dbbd0b;border-color:#dbbd0b}header *{font-family:proxima-nova,HelveticaNeue,"Helvetica Neue",Helvetica,Verdana,Arial,sans-serif;font-weight:900}.left{text-align:left}.right{text-align:right}.center,figure,figure figcaption,figure figcaption p{text-align:center}.f-l{float:left}#top,.f-r{float:right}.c-l{clear:left}.c-r{clear:right}.c-b{clear:both}.i-b{display:inline-block}.a-c,header ul{text-transform:uppercase}.s-c{font-variant:small-caps}body{background:#f3f3f3}body.classification{background:#faf4f8}body.regression{background:#edf1f6}body.techniques{background:#f3f6f2}body.summary{background:#f9f9f4}.container{margin:0 auto;width:960px}.post-html{width:640px;margin:0 auto;padding:0 0 36px}.post-html ol{list-style:decimal-leading-zero}.post-html ol li{padding-bottom:10px;line-height:1.4em}.post-html ul{float:none;list-style:disc}.post-html ul li{padding-bottom:10px;line-height:1.4em}.post-html ul li>ul{padding-left:24px;padding-top:6px}.post-html ul li>ul>li{list-style:circle;padding-bottom:4px}header{margin:10px 0 26px;padding:0 40px}header *{display:inline-block}.titles{position:relative;margin-top:-460px}footer{height:18px;margin:18px 0 36px;display:block;width:960px}figure{padding:0 0 18px;margin:0 auto}figure img{padding:9px;width:520px}figure figcaption{width:100%;font-size:75%;color:#949494;position:relative}#top{position:fixed;bottom:24px;right:12%}nav{display:inline-block;height:80px;float:left}nav li{cursor:pointer}header ul{float:left}header ul>li{display:inline-block;padding:32px 0 0 24px;position:relative}header ul>li>ul{float:right;height:24px;margin:-9px 16px 0}header ul>li>ul li{padding:0 16px 0 0}header ul>li>ul li .classification{color:#cd9aba}header ul>li>ul li .regression{color:#8baecc}header ul>li>ul li .techniques{color:#669c57}header ul>li>ul li .summary{color:#e3cf56}header ul>li>ul li:hover .classification{color:#ab568a}header ul>li>ul li:hover .regression{color:#4d86b4}header ul>li>ul li:hover .techniques{color:#277512}header ul>li>ul li:hover .summary{color:#dbbd0b}dl{clear:both}dl .defn-container{float:left;width:480px;height:80px;border-left:4px solid #272727;padding:9px 0 9px 9px;margin:9px;background:#fff;-webkit-border-top-right-radius:3px;-webkit-border-bottom-right-radius:3px;-moz-border-radius-topright:3px;-moz-border-radius-bottomright:3px;border-top-right-radius:3px;border-bottom-right-radius:3px}dl .defn-container.classification{border-color:#881058}dl .defn-container.regression{border-color:#0e5f9b}dl .defn-container.techniques{border-color:#277512}dl .defn-container.summary{border-color:#dbbd0b}dl dt{margin-bottom:18px;width:440px}dl dd{font-size:.75em;line-height:1.125em;width:440px}dl .defn-arr{padding-top:32px;float:right;display:inline-block}dl .defn-arr a{text-decoration:none}dl:after{clear:left}.vertical-bar{position:relative;top:5px;height:24px;width:4px;margin-right:6px;display:inline-block}.vertical-bar.tall{position:static;height:80px;margin:0 16px}.vertical-bar.dark-grey{background:#272727}.vertical-bar.classification{background:#cd9aba}.vertical-bar.regression{background:#8baecc}.vertical-bar.techniques{background:#669c57}.vertical-bar.summary{background:#e3cf56}a.purple:hover .vertical-bar{background:#ab568a}a.blue:hover .vertical-bar{background:#4d86b4}a.green:hover .vertical-bar{background:#277512}a.yellow:hover .vertical-bar{background:#dbbd0b}hr{border:none;border-top:1px solid #3e3e3e}.classification hr{border-color:#881058}.regression hr{border-color:#0e5f9b}.techniques hr{border-color:#277512}.summary hr{border-color:#dbbd0b}table{border-collapse:collapse;border-spacing:0;empty-cells:show;border:2px solid #3e3e3e;margin:15px auto}.classification table{border-color:#881058}.regression table{border-color:#0e5f9b}.techniques table{border-color:#277512}.summary table{border-color:#dbbd0b}table caption{color:#000;font:italic 85%/1 arial,sans-serif;padding:1em 0;text-align:center}table td,table th{border-width:0 0 0 1px;font-size:inherit;text-align:center;margin:0;overflow:visible;padding:.5em 1em}table td:first-child,table th:first-child{border-left-width:0}table thead{text-align:left;vertical-align:bottom}.classification table thead{background-color:#881058;color:#faf4f8}.regression table thead{background-color:#0e5f9b;color:#edf1f6}.techniques table thead{background-color:#277512;color:#f3f3f3}.summary table thead{background-color:#dbbd0b;color:#f3f3f3}table td{background-color:transparent}.classification table tr:nth-child(2n) td{background-color:#cd9aba}.regression table tr:nth-child(2n) td{background-color:#8baecc}.techniques table tr:nth-child(2n) td{background-color:#669c57}.summary table tr:nth-child(2n) td{background-color:#e3cf56}@-webkit-keyframes fadeInLeft{0%{opacity:0;-webkit-transform:translate3d(-16px,0,0);transform:translate3d(-16px,0,0)}100%{opacity:1;-webkit-transform:none;transform:none}}@keyframes fadeInLeft{0%{-webkit-transform:translate3d(-16px,0,0);transform:translate3d(-16px,0,0)}100%{opacity:1;-webkit-transform:none;transform:none}}.fadeInLeft{-webkit-animation:fadeInLeft .4s;animation:fadeInLeft .4s;-webkit-animation-fill-mode:both}
  </style>

  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->


</head>
<body class="summary">

  <div class="container">
      <img src="elliotImages/homepage-background.png" alt="Large graph image" width="960"/>
  <div class="titles">
    <h1 class="center">
      Music Emotion Recognition
    </h1>
    <h2 class="center">
      Machine Learning
    </h2>
  </div>
  <div id="demo" class="post-html"><!-- - What is MER?
    - Sub-category of MIR study
    - Define emotion
   - Cover techniques &amp; problems
   - History of MER
    - Disclaimer: New science so not a lot of history
    - What sparked interest
  #### Non-Textual Features
    - Schematic of one of the techniques (Flow diagram)
    -->

  <h4 class="center">"Without music, life would be a mistake" Friedrich Nietzsche, German philosopher.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref">1</a></sup></h4>

  <p><br></p>

  <p>Over the last decade the availability of music has increased dramatically.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref">2</a></sup> Gone are the days of hovering over the record button on a tape deck to record this week's hits from the radio, nowadays people can have multiple music libraries synced across multiple devices sometimes using multiple applications. Due to the increased public use of music applications the research in the music field has also increased in the last few years. Research has been focused into applications of <abbr title="Music Emotion Recognition">MER</abbr> and also into how we can use machines to automate the laborious, expensive work humans would otherwise have to do. Although the techniques aren't perfect yet users are starting to see the research appear in everyday applications, iTunes has &lsquo;Genius&rsquo;, Rdio has a &lsquo;Browse&rsquo; by feeling or mood section and there are countless more examples to add to the list. On this website we will talk about the techniques used to allow these new features to be used by users. More specifically how we can use machines to do our work, and not only do the work but act&shy;ually take a small amount of data and use all of its previous efforts to make increasingly more accurate predictions.</p>

  <h4 id="Fig11" class="center">Full <abbr title="Music Emotion Recognition">MER</abbr> Process</h4>

  <p><figure markdown="1">
  <img src="elliotImages/MER-Flowchart.png" alt="MER Process" /></p>

  <figcaption>

  <p>Figure 1.1: Above is the entire process of Music Emotion Recognition, with our concentration, Machine Learning, highlighted in&nbsp;blue.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref">3</a></sup></p>

  </figcaption>

  <p></figure></p>

  <p><!-- History -->
  Music Emotion Recognition (<abbr title="Music Emotion Recognition">MER</abbr>) has stemmed from a group of research called Music Information Retrieval (<abbr title="Music Information Retrieval">MIR</abbr>) which has a rapidly expanding community spanning across a myriad of disciplines including: information science, musicology, audio engineering, computer science and business<sup id="fnref:1"><a href="#fn:1" class="footnote-ref">4</a></sup> <sup id="fnref:2"><a href="#fn:2" class="footnote-ref">5</a></sup>. <abbr title="Music Emotion Recognition">MER</abbr> has been identified as a &ldquo;powerful&rdquo; way of organising the sheer amount of ever increasing music information in a way that can be easily accessed<sup id="fnref2:2"><a href="#fn:2" class="footnote-ref">5</a></sup>. To be able to accurately categorise this musical information we need to have machines that are capable of processing the volume of information that gets created everyday; it would be simply too much work for humans to do. This is where applying the Machine Learning (<abbr title="Machine Learning">ML</abbr>) models to the <abbr title="Music Emotion Recognition">MER</abbr> processes comes in.</p>

  <p><!-- Techniques and problems -->
  On this website we will cover two of the most common <abbr title="Music Emotion Recognition">MER</abbr> processes <em><a href="articles/classification">Classification</a></em> &amp; <em><a href="articles/regression">Regression</a></em>. We will also discuss three <abbr title="Machine Learning">ML</abbr> methods on the  <em><a href="articles/techniques">Techniques</a></em> page: <a href="articles/techniques#SVR">Support Vector Regression (<abbr title="Support Vector Regression">SVR</abbr>)</a>, <a href="articles/techniques#SVM">Support Vector Machines (<abbr title="Support Vector Machines">SVM</abbr>)</a> &amp; <a href="articles/techniques#NN">Neural Networks</a> (NN).</p>

  <h4>Support Vector Regression</h4>

  <p>Support Vector Regression works by taking the musical properties, such as harmonics &amp; rhythm etc. and maps them onto a Valence-Arousal Diagram (See <a href="#Fig12">Fig. 1.2</a>). The emotion that has been chosen by the machine is then represented as 2D Vector in the form (<em>a</em>, <em>v</em>).</p>

  <h4 id="Fig12" class="center">Valence-Arousal Diagram</h4>

  <p><figure markdown="1">
  <img src="elliotImages/VADiagram.png" alt="Thayer's two-dimensional emotion plane" /></p>

  <figcaption>

  <p>Figure 1.2: Adapted from: Russell, J. A. (1980). A circumplex model of affect. Journal of Personality and Social Psychology, 39, 1161–1178.</p>

  </figcaption>

  <p></figure></p>

  <h4>Support Vector Machines</h4>

  <p>Support Vector Machines take training data, called <em>observations</em>. This data to make better predictions about future data that is given to the machine without ground truths. <abbr title="Support Vector Machines">SVM</abbr> work by plotting the test vectors in a <code>n</code> dimensional set space, where the number of dimensions, <code>n</code>, comes from the number of musical features (in this case) taken into consideration; tempo, rhythm, volume, lyrics etc. A hyperplane is then created to split the data into 2 classifications (<code>+1</code> or <code>-1</code>), the hyperplane must create the maximum margin between itself and the closest vectors (<em>Support Vectors</em>) on the positive side and the negative side - <a href="#Fig13">Figure 1.3</a> demonstrates this process. The machine then looks at the data to see if it matches the ground truth and alters a weighting coefficient until the partitioned data matches the ground truth.</p>

  <h4 id="Fig13" class="center">Creating the Hyperplane</h4>

  <p><figure markdown="1">
  <img src="elliotImages/MER-SVM-Process.gif" alt="Creating the Hyperplane" /></p>

  <figcaption>

  <p>Figure 1.3: The Process of creating the Hyperplane<sup id="fnref:7"><a href="#fn:7" class="footnote-ref">6</a></sup></p>

  </figcaption>

  <p></figure></p>

  <h4>Neural Networks</h4>

  <p>Neural Networks is modelled on our biological nervous system. It works by associating attri&shy;butes and characteristics of the data with the output - in <abbr title="Music Emotion Recognition">MER</abbr> that output can be a valence value and an arousal value. The network is set up with an input layer of musical features and a hidden layer of musical features, with weighted associations, <code>W</code><sub><code>hi</code></sub>,  between the two sets of nodes. Then joining the hidden layer to the output layer is a another set of weighted associations, <code>W</code><sub><code>oh</code></sub>. Values are computed by picking initial random values close to <code>0</code> and processing them the values at each layer (a combination of passing them through sigmoidal functions, summing the values &amp; multipling by  <code>W</code><sub><code>hi</code></sub> or <code>W</code><sub><code>oh</code></sub>).<sup id="fnref:4"><a href="#fn:4" class="footnote-ref">7</a></sup></p>

  <p><figure markdown="1">
  <img src="elliotImages/NN_explanation.png" alt="Neural Network schema" /></p>

  <figcaption>

  <p>Figure 1.4: Neural Network of the Experience<sup id="fnref2:4"><a href="#fn:4" class="footnote-ref">7</a></sup></p>

  </figcaption>

  <p></figure></p>

  <p>The output is then compared to the correct values, a backpropagation algorithm is then applied which reduces the errors. The machine is then trained by repeating the above steps, which can be around 10,&nbsp;000 cycles.<sup id="fnref3:4"><a href="#fn:4" class="footnote-ref">7</a></sup></p>

  <p>More information on the three methods can be found by reading our <em><a href="articles/techniques">Techniques</a></em> page.</p>

  <p><em>NB: Top background image adapted from a series of experiments described in Bachorik et al. (2009).</em></p>

  <h3>References</h3>

  <div class="footnotes">
  <hr />
  <ol>

  <li id="fn:3">
  <p>Trohidis, K., Tsoumakas, G., Kalliris, G. &amp; Vlahavas, I. (2011) Multi-label classification of music by emotion. EURASIP Journal on Audio, Speech, and Music Processing. [Online] 2011 (1), 325-327. Available from: <a href="https://books.google.co.uk/books?hl=en&amp;lr=&amp;id=OHp3sRnZD-oC&amp;oi=fnd&amp;pg=PA325&amp;dq=Multi-Label+Classification+of+Music+into+Emotions.&amp;ots=oELPqJiBg3&amp;sig=Kzh-y51zNuA1AomQzAn8MRffx10#v=onepage&amp;q=Multi-Label%20Classification%20of%20Music%20into%20Emotions.&amp;f=false">https://books.google.co.uk/books?hl=en&amp;lr=&hellip;</a> [Accessed: 10 March 2015].&#160;<a href="#fnref:3" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
  </li>

  <li id="fn:5">
  <p>Kim, Y., Schmidt, E., Migneco, R., Morton, B., Richardson, P., Scott, J., Speck, J. and Turnbull, D. (2015). Music Emotion Recognition: A State of the Art Review. [Online] Available at: <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.231.7740&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/downlo&hellip;</a> [Accessed 13 Mar. 2015].&#160;<a href="#fnref:5" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
  </li>

  <li id="fn:6">
  <p>Adapted from: Figure 1.1 of Schmidt, E. and Kim, Y. (2015). Modeling and Predicting Emotion in Music. [Online] Available at: <a href="http://www.tcnj.edu/~mmi/papers/Paper52.pdf">http://www.tcnj.edu/~mmi/papers/Paper52.pdf</a> [Accessed 13 Mar. 2015].&#160;<a href="#fnref:6" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
  </li>

  <li id="fn:1">
  <p>Downie, J. (2003) Music information retrieval. Annual Review of Information Science and Technology. [Online] 37 (1), 295-296. Available from: <a href="http://onlinelibrary.wiley.com/doi/10.1002/aris.1440370108/full">http://onlinelibrary.wil&hellip;</a> [Accessed: 10 March 2015].&#160;<a href="#fnref:1" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
  </li>

  <li id="fn:2">
  <p>Yang, Y., Lin, Y., Su, Y. &amp; Chen, H. (2008) A Regression Approach to Music Emotion Recognition. IEEE Transactions on Audio, Speech, and Language Processing. [Online] 16 (2), 448. Available from: <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4432654">http://ieeexplore.ieee.org/stamp/sta&hellip;</a> [Accessed: 10 March 2015].&#160;<a href="#fnref:2" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref2:2" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
  </li>

  <li id="fn:7">
  <p>Adapted from: Moore, A. (2003) Support Vector Machines [Online] Available at: <a href="http://www.cs.cmu.edu/~cga/ai-course/svm.pdf">http://www.cs.cmu.edu/~cga/ai-course/svm.pdf</a> Slides 5-9&#160;<a href="#fnref:7" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
  </li>

  <li id="fn:4">
  <p>Vempala, N. and Russo, F. (2012). Predicting emotion from music audio features using neural networks. Proceedings of the 9th International Symposium on Computer Music Modeling and Retrieval (CMMR). [Online] Available at: <a href="http://www.cmmr2012.eecs.qmul.ac.uk/sites/cmmr2012.eecs.qmul.ac.uk/files/pdf/papers/cmmr2012_submission_66.pdf">http://www.cmmr2012.eecs.qmul.ac.uk/sit&hellip;</a> [Accessed 10 Mar. 2015].&#160;<a href="#fnref:4" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref2:4" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref3:4" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
  </li>

  </ol>
  </div>
  <div id="post-summary" class="post-html"><h1>Summary</h1>

  <hr>

  <h3>Introduction</h3>

  <p>On this website we have discussed various Machine Learning (<abbr title="Machine Learning">ML</abbr>) techniques, such as: Support Vector Machines (<abbr title="Support Vector Machine">SVM</abbr>), Support Vector Regression (<abbr title="Support Vector Regression">SVR</abbr>) &amp; Neural Networks (NN) (<a href="techniques">Techniques</a>), as well as Linear Regression and Fuzzy k—Nearest Neighbour (on <a href="regression">Regression</a> &amp; <a href="classification">Classification</a> respectively). In this section we will take a look at which of these methods, if any, stand out from the rest in terms of accuracy, the number of observations in needs to make a reasonable prediction, how well they help with Music Emotion Recognition (<abbr title="Music Emotion Recognition">MER</abbr>) in particular et al.</p>

  <p>Before going ahead with the conclusions, it should be pointed out that <abbr title="Machine Learning">ML</abbr> is most definitively a much more effective method of establishing the emotion of a piece of music. While humans can <em>very</em> accurately say what the emotion of a piece of music is quickly, and can do this at any stage of the piece, we can only do it for small number of samples. When you need 1,000 samples analysing, humans will begin to slow down, take breaks and become restless. This human behaviour will result in a gradual loss of precision in the answers given. For this reason machines are better suited for the sheer quantity of data we want to handle when carrying out these studies.</p>

  <h3>Comparisons</h3>

  <p>We shall begin by looking at accuracy of the methods. Recall from the <a href="techniques">techniques</a> page <a href="#Fig51">Fig. 5.1</a>. From this table it seems that Gaussian Mixture Models (<abbr title="Gaussian Mixture Model">GMM</abbr>) is very accurate - though still incorrectly identifying 1-in-10.</p>

  <p></p><figure markdown="1"><p></p>

  <h4 id="Fig51">Accuracy</h4>

  <table>
  <thead>
  <tr>
    <th>Technique</th>
    <th>Accuracy</th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td><abbr title="Support Vector Regression">SVR</abbr></td>
    <td>78.8%</td>
  </tr>
  <tr>
    <td><abbr title="Support Vector Machine">SVM</abbr></td>
    <td>60.5%</td>
  </tr>
  <tr>
    <td><strong><abbr title="Gaussian Mixture Model">GMM</abbr></strong></td>
    <td><strong>92.1%</strong></td>
  </tr>
  <tr>
    <td>Neural Network</td>
    <td>85.6%</td>
  </tr>
  <tr>
    <td>KNN</td>
    <td>38.9%</td>
  </tr>
  </tbody>
  </table>

  <figcaption>

  <p>Figure 5.1: Comparative Table of Various <abbr title="Machine Learning">ML</abbr> Techniques Applied in <abbr title="Music Emotion Recognition">MER</abbr></p>

  </figcaption>

  <p></p></figure><p></p>

  <p>From the table we can see that NN's accuracy is not too dissimilar from <abbr title="Gaussian Mixture Model">GMM</abbr>'s, ~7% less accurate. However, when used in combination <abbr title="Gaussian Mixture Model">GMM</abbr> and NN create both an accurate and computationally efficient process for emotional taxonomy<sup id="fnref:1"><a href="#fn:1" class="footnote-ref">1</a></sup>. One additional note is that by only verifying and subset of the data when using <abbr title="Gaussian Mixture Model">GMM</abbr> you can further <em>significantly</em> reduce the computational cost<sup id="fnref2:1"><a href="#fn:1" class="footnote-ref">1</a></sup>.</p>

  <p>The issue with some of these methods, however, is that the output is a vector; most commonly a Valence and Arousal value, from the Thayer's 2D emotional plane. This is difficult for humans to understand, for this reason it can be argued that <a href="classification">classification</a> is preferable to <a href="regression">regression</a> as when used in applications the user can directly and instinctively know what the output means, viz. out of "Sad" and "(-0.6, -0.25)", "Sad" is instantly more recognisable.</p>

  <!-- Regression -->

  <h3>Other Machine Learning Techniques</h3>

  <p>Fuzzy labelling appears to be a promising method of emotional taxonomy. It offers both the advantages of being able to offer output that can be easily understood by humans as well as the psuedo-continuous nature of the output lending itself nicely to being ordered. This orderable property of the output means when used in applications it offers up alternatives. In the majority of cases the top result will correlate with the users opinion, however the other <a href="classification#FKNN">30%</a> of the time it may be one of the other outputs offered by the machine; meaning you reduce the <em>apparent</em> error, due to <a href="regression#MisC">Miscommunication</a> &amp; <a href="regression#PsyI">Psychological Impact</a>, that the end user perceives because of their personal tastes and experiences. While there is further work to be done on the Fuzzy K-Nearest Neighbour (FKNN) <abbr title="Machine Learning">ML</abbr> technique, before it has similar accuracy levels as something like <abbr title="Support Vector Machine">SVM</abbr> &amp; <abbr title="Gaussian Mixture Model">GMM</abbr>, it still has potential to be a strong candidate if incorporated into applications. This is similarly true for other fuzzy <abbr title="Machine Learning">ML</abbr> techniques such as Fuzzy Nearest-Mean which actually has a slightly better accuracy of 78.33%<sup id="fnref:2"><a href="#fn:2" class="footnote-ref">2</a></sup>.</p>

  <h3>Conclusion</h3>

  <p>On this website, we have discussed Machine Learning and how it aids Music Emotion Recognition in establishing the emotion/mood of a song segment. We've looked at how these <abbr title="Machine Learning">ML</abbr> techniques can overcome several problems including: human perception by looking into <a href="classification#FKNN">fuzzy labelling</a>, generating accurate results from very little data by learning from similar examples it has seen previously using <a href="regression#ActLearning">active learning</a>, and detecting changes in emotion throughout the course of a song using <a href="regression#TimeCont">time-continuous regression</a>.</p>

  <!--
  - Potential Machine Learning techniques that haven't been used a great deal, but may be useful in solving certain problems
    - GMM
    - Decision Tree Learning
    - K-Nearest Neighbours

  -->

  <h3>References</h3>

  <div class="footnotes">
  <hr>
  <ol>

  <li id="fn:1">
  <p>Bing Xiang, and Berger, T. (2003). Efficient text-independent speaker verification with structu­ral gaussian mixture models and neural network. IEEE Transactions on Speech and Audio Processing, [online] 11(5), pp.447-456. Available at: <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1223594&amp;tag=1">http://ieeexplore.ieee.org/xpls/abs_all…</a> [Acc­essed 16 Mar. 2015].&nbsp;<a href="#fnref:1" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref2:1" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
  </li>

  <li id="fn:2">
  <p>Yang, Y., Liu, C. and Chen, H. (2006). Music emotion classification. Proceedings of the 14th annual ACM international conference on Multimedia - MULTIMEDIA '06. [online] Available at: <a href="http://delivery.acm.org/10.1145/1190000/1180665/p81-yang.pdf?ip=129.31.74.197&amp;id=1180665&amp;acc=ACTIVE%20SERVICE&amp;key=BF07A2EE685417C5%2EF5014A9D3D5CC2D9%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;CFID=640746836&amp;CFTOKEN=80988680&amp;__acm__=1426551374_0a116eecd6560832218d5b22b6b2c4dd">http://delivery.acm.org/10.1145/11…</a> [Accessed 17 Mar. 2015].&nbsp;<a href="#fnref:2" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
  </li>

  </ol>
  </div>
  </div>
</body>
</html>
