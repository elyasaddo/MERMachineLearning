<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="ie ie6" lang="en"> <![endif]-->
<!--[if IE 7 ]><html class="ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html lang="en"> <!--<![endif]-->
<head>

  <!-- Basic Page Needs
  ================================================== -->
  <meta charset="utf-8">
  <title>Elyas Addo's Personal Page</title>
  <meta name="description" content="Personal Page">
  <meta name="author" content="Elyas Addo">


  <!-- Mobile Specific Metas
  ================================================== -->
  <!--<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">-->
  <script src="//use.typekit.net/xvl2gac.js"></script>
  <script>try{Typekit.load();}catch(e){}</script>

  <!-- CSS
  ================================================== -->
  <style>
  #top,a,abbr,acronym,address,applet,article,aside,audio,b,big,blockquote,body,canvas,caption,center,cite,code,dd,del,details,dfn,div,dl,dl dt,dt,em,embed,fieldset,figcaption,figure,footer,form,h1,h2,h3,h4,h5,h6,header,hgroup,html,i,iframe,img,ins,kbd,label,legend,li,mark,menu,nav,object,ol,output,p,pre,q,ruby,s,samp,section,small,span,strike,strong,sub,summary,sup,table,tbody,td,tfoot,th,thead,time,tr,tt,u,ul,var,video{margin:0;padding:0;border:0;font:inherit;vertical-align:baseline}article,aside,details,figcaption,figure,footer,header,hgroup,menu,nav,section{display:block}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:after,blockquote:before,q:after,q:before{content:'';content:none}::selection{color:#050505;background:#c7c7c7}::-moz-selection{color:#050505;background:#c7c7c7}html{color:#272727;font-family:Calluna,Georgia,Times,"Times New Roman",serif;font-variant-ligatures:additional-ligatures;-webkit-font-variant-ligatures:additional-ligatures;text-rendering:optimizeLegibility;font-kerning:normal}p{margin:0 0 18px;font-size:1em;line-height:1.4em;text-align:justify;-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;hyphens:auto}p.lead{font-size:1em;line-height:1.5em}h1{font-size:3em;line-height:1.125em}h2{font-size:2.25em;line-height:1.5em}#top,h3{font-size:1.5em;line-height:2em;font-weight:700}h4{font-size:1em;line-height:1.5em;font-weight:700}h5{line-height:1.5em;font-weight:700}a,a:visited{color:#050505}a:focus,a:hover{text-decoration:none}a:focus.purple,a:hover.purple{color:#cd9aba}a:focus.blue,a:hover.blue{color:#8baecc}a:focus.green,a:hover.green{color:#a4c39c}a:focus.yellow,a:hover.yellow{color:#e3cf56}nav a,nav a:visited{text-decoration:none;color:#050505}.classification header>nav a:focus,.classification header>nav>ul>li>a:hover{color:#881058}.regression header>nav a:focus,.regression header>nav>ul>li>a:hover{color:#0e5f9b}.techniques header>nav a:focus,.techniques header>nav>ul>li>a:hover{color:#277512}.summary header>nav a:focus,.summary header>nav>ul>li>a:hover{color:#dbbd0b}.home header>nav a:focus,.home header>nav>ul>li>a:hover,.listen header>nav a:focus,.listen header>nav>ul>li>a:hover{color:#c7c7c7}em{font-style:italic}dl dt,h1,h2,strong{font-weight:700}code,pre{font-family:prestige-elite-std,Menlo,monospace}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sup{top:-.5em}sub{bottom:-.25em}blockquote{margin:10px 50px 10px 0;padding-left:15px;border-left:3px solid #c7c7c7}.classification blockquote{color:#881058;border-color:#881058}.regression blockquote{color:#0e5f9b;border-color:#0e5f9b}.techniques blockquote{color:#277512;border-color:#277512}.summary blockquote{color:#dbbd0b;border-color:#dbbd0b}header *{font-family:proxima-nova,HelveticaNeue,"Helvetica Neue",Helvetica,Verdana,Arial,sans-serif;font-weight:900}.left{text-align:left}.right{text-align:right}.center,figure,figure figcaption,figure figcaption p{text-align:center}.f-l{float:left}#top,.f-r{float:right}.c-l{clear:left}.c-r{clear:right}.c-b{clear:both}.i-b{display:inline-block}.a-c,header ul{text-transform:uppercase}.s-c{font-variant:small-caps}body{background:#f3f3f3}body.classification{background:#faf4f8}body.regression{background:#edf1f6}body.techniques{background:#f3f6f2}body.summary{background:#f9f9f4}.container{margin:0 auto;width:960px}.post-html{width:640px;margin:0 auto;padding:0 0 36px}.post-html ol{list-style:decimal-leading-zero}.post-html ol li{padding-bottom:10px;line-height:1.4em}.post-html ul{float:none;list-style:disc}.post-html ul li{padding-bottom:10px;line-height:1.4em}.post-html ul li>ul{padding-left:24px;padding-top:6px}.post-html ul li>ul>li{list-style:circle;padding-bottom:4px}header{margin:10px 0 26px;padding:0 40px}header *{display:inline-block}.titles{position:relative;margin-top:-460px}footer{height:18px;margin:18px 0 36px;display:block;width:960px}figure{padding:0 0 18px;margin:0 auto}figure img{padding:9px;width:520px}figure figcaption{width:100%;font-size:75%;color:#949494;position:relative}#top{position:fixed;bottom:24px;right:12%}nav{display:inline-block;height:80px;float:left}nav li{cursor:pointer}header ul{float:left}header ul>li{display:inline-block;padding:32px 0 0 24px;position:relative}header ul>li>ul{float:right;height:24px;margin:-9px 16px 0}header ul>li>ul li{padding:0 16px 0 0}header ul>li>ul li .classification{color:#cd9aba}header ul>li>ul li .regression{color:#8baecc}header ul>li>ul li .techniques{color:#669c57}header ul>li>ul li .summary{color:#e3cf56}header ul>li>ul li:hover .classification{color:#ab568a}header ul>li>ul li:hover .regression{color:#4d86b4}header ul>li>ul li:hover .techniques{color:#277512}header ul>li>ul li:hover .summary{color:#dbbd0b}dl{clear:both}dl .defn-container{float:left;width:480px;height:80px;border-left:4px solid #272727;padding:9px 0 9px 9px;margin:9px;background:#fff;-webkit-border-top-right-radius:3px;-webkit-border-bottom-right-radius:3px;-moz-border-radius-topright:3px;-moz-border-radius-bottomright:3px;border-top-right-radius:3px;border-bottom-right-radius:3px}dl .defn-container.classification{border-color:#881058}dl .defn-container.regression{border-color:#0e5f9b}dl .defn-container.techniques{border-color:#277512}dl .defn-container.summary{border-color:#dbbd0b}dl dt{margin-bottom:18px;width:440px}dl dd{font-size:.75em;line-height:1.125em;width:440px}dl .defn-arr{padding-top:32px;float:right;display:inline-block}dl .defn-arr a{text-decoration:none}dl:after{clear:left}.vertical-bar{position:relative;top:5px;height:24px;width:4px;margin-right:6px;display:inline-block}.vertical-bar.tall{position:static;height:80px;margin:0 16px}.vertical-bar.dark-grey{background:#272727}.vertical-bar.classification{background:#cd9aba}.vertical-bar.regression{background:#8baecc}.vertical-bar.techniques{background:#669c57}.vertical-bar.summary{background:#e3cf56}a.purple:hover .vertical-bar{background:#ab568a}a.blue:hover .vertical-bar{background:#4d86b4}a.green:hover .vertical-bar{background:#277512}a.yellow:hover .vertical-bar{background:#dbbd0b}hr{border:none;border-top:1px solid #3e3e3e}.classification hr{border-color:#881058}.regression hr{border-color:#0e5f9b}.techniques hr{border-color:#277512}.summary hr{border-color:#dbbd0b}table{border-collapse:collapse;border-spacing:0;empty-cells:show;border:2px solid #3e3e3e;margin:15px auto}.classification table{border-color:#881058}.regression table{border-color:#0e5f9b}.techniques table{border-color:#277512}.summary table{border-color:#dbbd0b}table caption{color:#000;font:italic 85%/1 arial,sans-serif;padding:1em 0;text-align:center}table td,table th{border-width:0 0 0 1px;font-size:inherit;text-align:center;margin:0;overflow:visible;padding:.5em 1em}table td:first-child,table th:first-child{border-left-width:0}table thead{text-align:left;vertical-align:bottom}.classification table thead{background-color:#881058;color:#faf4f8}.regression table thead{background-color:#0e5f9b;color:#edf1f6}.techniques table thead{background-color:#277512;color:#f3f3f3}.summary table thead{background-color:#dbbd0b;color:#f3f3f3}table td{background-color:transparent}.classification table tr:nth-child(2n) td{background-color:#cd9aba}.regression table tr:nth-child(2n) td{background-color:#8baecc}.techniques table tr:nth-child(2n) td{background-color:#669c57}.summary table tr:nth-child(2n) td{background-color:#e3cf56}@-webkit-keyframes fadeInLeft{0%{opacity:0;-webkit-transform:translate3d(-16px,0,0);transform:translate3d(-16px,0,0)}100%{opacity:1;-webkit-transform:none;transform:none}}@keyframes fadeInLeft{0%{-webkit-transform:translate3d(-16px,0,0);transform:translate3d(-16px,0,0)}100%{opacity:1;-webkit-transform:none;transform:none}}.fadeInLeft{-webkit-animation:fadeInLeft .4s;animation:fadeInLeft .4s;-webkit-animation-fill-mode:both}
  </style>

  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->


</head>
<body class="classification">

  <div id="post-classification" class="post-html"><h1>Classification</h1>

<hr>

<h3>Introduction</h3>

<h4>What is a classification problem?</h4>

<p>A classification problem is one in which the data is to be given some discrete label(s) describing their nature in relation to a given situation. In the context of Music Emotion Recognition (MER) this involves giving a piece of music a label or a set of labels which describe which emotion(s) the music is trying to evoke, e.g. confident, cheerful, aggressive. See <a href="#Fig31">Fig. 3.1</a> for the MIREX table of 5 clusters of emotional labels.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref">1</a></sup> It is important to note that what the Machine Learning Algorithms want to retrieve is the emotion which the piece of music is trying to evoke, rather than the perceived emotion, as listeners may respond to different pieces of music in different way, e.g. a person may be pleased to hear melancholic music if this is the tone of music they enjoy.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref">2</a></sup></p>

<p>Ground truth is the term given to a label which a certain piece of data actually has. the aim of classification techniques is to suggest a high probability (or in a perfect world 'certainty') of the data having this ground truth label. Note that many ground truths can exist for a given data piece.</p>

<p></p><figure markdown="1"><p></p>

<h4 id="Fig31">Cluster Table</h4>

<table>
<thead>
<tr>
  <th>Cluster 1</th>
  <th>Cluster 2</th>
  <th>Cluster 3</th>
  <th>Cluster 4</th>
  <th>Cluster 5</th>
</tr>
</thead>
<tbody>
<tr>
  <td>Rowdy</td>
  <td>Amiable</td>
  <td>Literate</td>
  <td>Witty</td>
  <td>Volatile</td>
</tr>
<tr>
  <td>Rousing</td>
  <td>Sweet</td>
  <td>Wistful</td>
  <td>Humorous</td>
  <td>Fiery</td>
</tr>
<tr>
  <td>Confident</td>
  <td>Fun</td>
  <td>Bittersweet</td>
  <td>Whimsical</td>
  <td>Visceral</td>
</tr>
<tr>
  <td>Boisterous</td>
  <td>Rollicking</td>
  <td>Autumnal</td>
  <td>Wry</td>
  <td>Aggressive</td>
</tr>
<tr>
  <td>Passionate</td>
  <td>Cheerful</td>
  <td>Brooding</td>
  <td>Campy</td>
  <td>Tense/Anxious</td>
</tr>
<tr>
  <td></td>
  <td></td>
  <td>Poignant</td>
  <td>Quirky</td>
  <td>Intense</td>
</tr>
<tr>
  <td></td>
  <td></td>
  <td></td>
  <td>Silly</td>
  <td></td>
</tr>
</tbody>
</table>

<figcaption>

<p>Figure 3.1: MIREX table of 5 clusters of emotions.<sup id="fnref2:7"><a href="#fn:7" class="footnote-ref">1</a></sup></p>

</figcaption>

<p></p></figure><p></p>

<h4>Different types of classification</h4>

<p>In general, if a problem is labeled as a classification problem, it means that all elements in the data set should fall into one of two opposing categories, e.g. in the scenario of determining whether it is day or night in a photograph, each photo should be classified as 'day' or 'not day'.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref">3</a></sup> This is clearly not the case for MER as there are many emotional labels for a piece of music to fall under. In this article, four different forms of classification will be discussed which involve a large set of candidate labels:</p>

<h5>Single-Label</h5>

<p>The single-label technique acknowledges the possibility of data belonging to many labels. However, once the algorithm finds labels which it believes the data belongs to, a single label is chosen based on some criterion, e.g. which label has the highest calculated probability for this data piece<sup id="fnref:4"><a href="#fn:4" class="footnote-ref">4</a></sup>. The training set of data in this type of classification are associated with one label each.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref">5</a></sup></p>

<h5>Multi-Class</h5>

<p>Multi-class classification involves creating a label which is the combination of two or more labels in the given set. It is like considering each member of the power set of labels as a possible fit for data, e.g. the set of all possible labels for the colour of a car include, but is not limited to: white; black; grey; blue; {white + black}; {black + blue + grey}<sup id="fnref2:4"><a href="#fn:4" class="footnote-ref">4</a></sup>.</p>

<h5>Multi-Label</h5>

<p>With multi-label classification, the output for the learning algorithm has such a form that it is possible to see if the data is thought to belong to each set. One way of doing this is by outputting a binary vector (sequence of 0s &amp; 1s) which act as flags to indicate whether each label fits.<sup id="fnref3:4"><a href="#fn:4" class="footnote-ref">4</a></sup> There are two types of classification problems: Problem Transformation Methods (PTMs) and Algorithm Adaptation Methods (AAMs). PTMs convert multi-label problems into one or more single-label problems, for which established and efficient algorithms already exist. AAMs take a more direct approach in which it takes a known machine learning (ML) algorithm and enhances it to cope with many labels.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref">6</a></sup></p>

<h5>Fuzzy Label</h5>

<p>The purposes of the previous forms mentioned are to output a definite value for whether data should be given a label or not. A fuzzy label, however, provides the probabilities of the data belonging to each category. The result from running a fuzzy classification algorithm can be thought of as similar to the binary vectors produced in multi-label classification, however the components can be any real number in the interval <code>[0,1]</code>. The non-discrete nature of a fuzzy label can be thought of as the stepping stone to continuous techniques of representing the emotion evoked by a piece of music like regression.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref">7</a></sup></p>

<h3>Why classification is difficult</h3>

<p>Classification is, in essence, pattern recognition - “the act of taking in raw data and making an action based on the "category" of the pattern”.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref">8</a></sup> Although humans clearly have this skill, we often struggle with defining rules for the classification of a particular set of data because it comes so naturally to us. Because of this, Machine Learning is very useful in classification as the rules created are data based, therefore more likely to be accurate in determining the nature of the data.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref">9</a></sup> The training set of data does however need to be labelled by a human before a machine can come up with a suitable model for classifying (known as supervised learning); otherwise it will just be trying to partition groups of data with common attributes (unsuprevised learning) which may not result in emotion related connections.<sup id="fnref:12"><a href="#fn:12" class="footnote-ref">10</a></sup></p>

<h3>Machine Learning Models</h3>

<h4>Support Vector Machines</h4>

<p>Support Vector Machines take training data (a.k.a. observations) and represent them as a pair; the first part of the pair is a vector, and the second is the ground truth. The vectors for each observation have n dimensions and are plotted in a set space also of <code>n</code> dimensions (<code>R</code><sup><code>n</code></sup>) where <code>n</code> is the number of elements / features used to compare the observations. Using one of the observations as the origin, hyperplanes are created to shatter the points (divide the points into two groups, data points which exist in the class and those which don't). If the data are not falling into the correct partition of the space, a vector of weighting coefficients is adjusted and applied to the the vectors to shift the data round in order to fit into the partition which matches their ground truth.<sup id="fnref:13"><a href="#fn:13" class="footnote-ref">11</a></sup></p>

<p></p><figure markdown="1">
<img src="/I/Imperial/Group%20Projects/Year%201%20-%20Topics/MERMachineLearning/assets/images/dataShattering.png" alt="Examples of data shattering in 2 dimensions" title="" id="Fig32"><p></p>

<figcaption>

<p>Figure 3.2: Examples of data shattering in 2 dimensions.<sup id="fnref2:13"><a href="#fn:13" class="footnote-ref">11</a></sup></p>

</figcaption>

<p></p></figure><p></p>

<p>Many hyperplanes can exist which shatter the data points correctly, SVMs choose the hyperplane which maximises the distance to the closest points in order to generate the most general model. The vectors closest to the hyper plane are known as the support vectors.<sup id="fnref:14"><a href="#fn:14" class="footnote-ref">12</a></sup> It is important to use a more general model as it is more resilient against subtle changes in test data and in real life applications.</p>

<p></p><figure markdown="1">
<img src="/I/Imperial/Group%20Projects/Year%201%20-%20Topics/MERMachineLearning/assets/images/manyHyperPlanes.png" alt="Demonstration of strongest hyperplane" title="" id="Fig33"><p></p>

<figcaption>

<p>Figure 3.3: Demonstration of strongest Hyperplane.<sup id="fnref2:14"><a href="#fn:14" class="footnote-ref">12</a></sup></p>

</figcaption>

<p></p></figure><p></p>

<p>One particular study into the use of SVMs for MER carried out by Chiang et al. used 35 features such as rhythm, dynamics and pitch to determine a model which fits pieces of music into four categories: happy, sad, peaceful and tensional. Once the relevant features had been extracted and selected, the intensity (arousal level) of the piece is classified as low or high using one SVM node, then the mood (valance level) of the piece is determined to decide which of the categories it belongs to. A flowchart for this system can be seen in <a href="#Fig34">Fig 3.4</a>. As we can see, SVMs can be used in traditional binary classification problems and part of PTMs for multi-label classification.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref">13</a></sup> Multi-class problems can also be approached with SVMs; however, since the number of classes will be large and sparse in the context of MER, it is not really used.<sup id="fnref4:4"><a href="#fn:4" class="footnote-ref">4</a></sup></p>

<p></p><figure markdown="1">
<img src="/I/Imperial/Group%20Projects/Year%201%20-%20Topics/MERMachineLearning/assets/images/ClassificationExampleFlowChart.png" alt="Flowchart of SVM usage in study by Chiang et. al." title="" id="Fig34"><p></p>

<figcaption>

<p>Figure 3.4: Flowchart of SVM usage in study by Chiang et. al.<sup id="fnref2:15"><a href="#fn:15" class="footnote-ref">13</a></sup></p>

</figcaption>

<p></p></figure><p></p>

<p>This study managed to achieve average accuracies of 86.94% in one data set and 92.33% in another.<sup id="fnref3:15"><a href="#fn:15" class="footnote-ref">13</a></sup></p>

<h4 id="FKNN">Fuzzy k-Nearest Neighbour Classifiers (FKNN)</h4>

<p>In regular k-Nearest Neighbour algorithms, the input data is predicted to exist in the category which it's k-Nearest Neighbours belong to. The <em>fuzzy</em> introduces the notion of likelihood in belonging to this category, and all categories.  To find the fuzzy membership <code>μ</code><!-- mu --> to a particular class of an input data, you take <code>(Σ(μ</code><sub><code>i</code></sub> <code>/</code> <code>x</code><sub><code>i</code></sub><sup><code>2</code></sup> <code>)) / (Σx</code><sub><code>i</code></sub><sup><code>-2</code></sup><code>) : 1 &lt;= i &lt;= k; x</code><sub><code>i</code></sub> is the distance from input data to data point <code>i</code>.</p>

<p>For the machine to create a model, it must first learn the values of the training set. These values are fuzzy vectors, of which, each component is calculated from the mean of the f<sup>th</sup> feature in each class. The machine learning part of this is when the error is calculated for each test data point and removes the weakest features are removed in order until the output vector accuracy converges. <!-- restructure this part #remove remove --></p>

<p>In a study carried out by Yang et al., each piece in the training set were split into segments. When labeling the segments, if the classifications were not a<!--?--> majority, the segment was withdrawn from the training data. This particular study found an accuracy of 70.88%<sup id="fnref:10"><a href="#fn:10" class="footnote-ref">14</a></sup></p>

<h4 id="GMM">Gaussian Mixture Model (GMM)</h4>

<p>GMM is another ML technique for single-label classification. Like SVM, the data points are feature vectors in an n-dimentional hyper-space where n is the number of musical features. GMM applies weightings to the features in order to form clusters of the data points. The data points are clustered in such a way that the probability of any data point being in a particular class is normally distributed across the hyper-space. The predicted class of any given data point is the class for which the probability is the highest in all of the normal distributions created by the model. Again, errors are calculated and the weightings are changed so that the model predictions and ground truths are reduced. For ML this process is repeated until the errors are irreducible.<sup id="fnref:17"><a href="#fn:17" class="footnote-ref">15</a></sup></p>

<p>In experiment an run by Han. et al. a GMM was found to produce an accuracy of 92.1%.<sup id="fnref:16"><a href="#fn:16" class="footnote-ref">16</a></sup></p>

<h3>Conclusion</h3>

<p>As you can see from the case studied above, Machine Learning is clearly far more effective in determining if a piece of music expresses a certain emotion than just flipping a coin. The problem of classification is greatly decreased by using ML techniques on large data sets. Humans would do a better job classifying a short list of songs; however, there is a greater efficiency introduced when a mathematical model is applied to the data.</p>

<p>As shown by the techniques above, there are different techniques to deal with the different forms of classification problems. <a href="#Fig35">Fig. 3.5</a> shows the advantages and disadvantages of using each of the four previously mentioned forms of classification in the context of MER.</p>

<p></p><figure markdown="1"><p></p>

<h4 id="Fig35">Classification Form Comparison</h4>

<table>
<thead>
<tr>
  <th>Form</th>
  <th>Advantages</th>
  <th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
  <td>Single-label</td>
  <td>It is clear to see what the primary emotion of the piece is</td>
  <td>All other emotions the music evoke are lost in the result</td>
</tr>
<tr>
  <td>Multi-class</td>
  <td>No emotional information is lost in the label given</td>
  <td>The possible set of labels grows exponentially with the number of classes. Also the labels are often sparse</td>
</tr>
<tr>
  <td>Multi-label</td>
  <td>The membership to each class is easily read and understood</td>
  <td>A lot of computational power is required for this as the membership to each class must be calculated</td>
</tr>
<tr>
  <td>Fuzzy-label</td>
  <td>The likelihood of membership to classes can make sorting by relevance easier</td>
  <td>The result may be less clear to read when working out if each piece belongs to a certain class</td>
</tr>
</tbody>
</table>

<figcaption>

<p>Figure 3.5: Comparison table for the different forms of classification in the context of MER.</p>

</figcaption>

<p></p></figure><p></p>

<p>Some may argue that, since certain emotions are so similar to each other and yet still distinct, the best way to express them is through some sort of continuous scale like the one you will see in the Regression article of this website. Though this may simplify a mathematical model, it is easier for a human who is stating or requesting the emotion of a piece of music to understand a clear label than than it is to understand a 3.6 on a certain emotion based axis. This is why classification is very effective in MER.</p>

<h3>References</h3>

<div class="footnotes">
<hr>
<ol>

<li id="fn:7">
<p>X., Downie, J., Laurier, C., Bay, M. and Ehmann, A. (2008). The 2007 MIREX audio mood classification task: Lessons learned. Proceedings of the International Conference on Music Information Retrieval. [online] Available at : <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.182.2004&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/…</a> [Accessed 5 Mar. 2015].&nbsp;<a href="#fnref:7" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref2:7" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:1">
<p>Yang, Y., Lin, Y., Su, Y. and Chen, H. (2008). A Regression Approach to Music Emotion Recognition. IEEE Transactions on Audio, Speech, and Language Processing, [online] 16(2), pp.448-457. Available at: <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.331.1655&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/…</a> [Accessed 12 Feb. 2015].&nbsp;<a href="#fnref:1" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:8">
<p>Ng,A.. 2012. Classification (8 min). [online]. [Accessed 25 Feb. 2015]. Available from: <a href="https://class.coursera.org/ml-005/lecture/33">https://class.coursera.org/ml-005/…</a>&nbsp;<a href="#fnref:8" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:4">
<p>Boutell, M. (2004). Learning multi-label scene classification*1. Pattern Recognition. [online] Available at: <a href="https://www.rose-hulman.edu/~boutell/publications/boutell04PRmultilabel.pdf">https://www.rose-hulman.edu/~boutell/…</a> [Accessed 3 Mar. 2015].&nbsp;<a href="#fnref:4" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref2:4" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref3:4" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref4:4" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:2">
<p>Trohidis, K., Tsoumakas, G., Kalliris, G. and Vlahavas, I. (2011). Multi-label classification of music by emotion. EURASIP Journal on Audio, Speech, and Music Processing, [online] 2011(1), p.4. Available at: <a href="http://ismir2008.ismir.net/papers/ISMIR2008_275.pdf">http://ismir2008.ismir.net/papers/…</a> [Accessed 13 Feb. 2015].&nbsp;<a href="#fnref:2" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:3">
<p>Tsoumakas, G. and Katakis, I. (2007). Multi-Label Classification. International Journal of Data Warehousing and Mining, [online] 3(3), pp.1-13. Available at: <a href="http://books.google.co.uk/books?hl=en&amp;lr=&amp;id=1bpEifVEi2MC&amp;oi=fnd&amp;pg=PA64&amp;dq=Multi-label+classification:An+overview&amp;ots=WyD83kziKF&amp;sig=P6VHFTT9RycLgfpCDrK0vq5o4hM#v=onepage&amp;q=single-label%20&amp;f=false">http://books.google.co.uk/books?hl…</a> [Accessed 15 Feb. 2015].&nbsp;<a href="#fnref:3" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:9">
<p>Barthet, M., Fazekas, G. and Sandler, M. (2013). Music emotion recognition: from content-to context-based models. From Sounds to Music and Emotions. [online] Available at: <a href="https://books.google.co.uk/books?id=zWG5BQAAQBAJ&amp;pg=PA243&amp;dq=fuzzy+label+classification&amp;hl=en&amp;sa=X&amp;ei=9dn2VIDsGIG3UeHngIgL&amp;ved=0CC4Q6AEwAA#v=onepage&amp;q=fuzzy%20label%20classification&amp;f=false">https://books.google.co.uk/books?id=zW…</a> [Accessed 5 Mar. 2015].&nbsp;<a href="#fnref:9" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:5">
<p>Duda, R., Hart, P. and Stork, D. (2001). Pattern classification. 2nd ed. New York: Wiley.&nbsp;<a href="#fnref:5" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:11">
<p>Schapire, R. (n.d.). Machine Learning Algorithms for Classification..&nbsp;<a href="#fnref:11" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:12">
<p>Ng,A.. 2012. Supervised Learning (12 min). [online]. [Accessed 25 Feb. 2015]. Available from: <a href="https://class.coursera.org/ml-005/lecture/3">https://class.coursera.org/ml-005/…</a>&nbsp;<a href="#fnref:12" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:13">
<p>Burges, C. J. (1998). A tutorial on support vector machines for pattern recognition. Data mining and knowledge discovery. [online] 2(2), 121-167. Available from: <a href="http://research.microsoft.com/pubs/67119/svmtutorial.pdf">http://research.microsoft.com/pubs/67119/svmtutorial.pdf</a> [Accessed 26 Feb. 2015].&nbsp;<a href="#fnref:13" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref2:13" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:14">
<p>opencv dev team, (2015). Introduction to Support Vector Machines — OpenCV 2.4.11.0 documentation. [online] Docs.opencv.org. Available at: <a href="http://docs.opencv.org/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html">http://docs.opencv.org/doc/…</a> [Accessed 17 Mar. 2015].&nbsp;<a href="#fnref:14" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref2:14" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:15">
<p>Chiang, W., Wang, J. and Hsu, Y. (2014). A Music Emotion Recognition Algorithm with Hierarchical SVM Based Classifiers. 2014 International Symposium on Computer, Consumer and Control. [online] Available at: <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6846115">http://ieeexplore.ieee.org/stamp/…</a> [Accessed 28 Feb. 2015].&nbsp;<a href="#fnref:15" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref2:15" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref3:15" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:10">
<p>Yang, Y., Liu, C. and Chen, H. (2006). Music emotion classification. Proceedings of the 14th annual ACM international conference on Multimedia - MULTIMEDIA '06. [online] Available at: <a href="http://dl.acm.org/citation.cfm?id=1180665">http://dl.acm.org/…</a> [Accessed 12 Feb. 2015].&nbsp;<a href="#fnref:10" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:17">
<p>Robertson, H. (2012) Introduction to Gaussian Mixture Models for Music Information Retrieval [Online] Available from: <a href="http://www.music.mcgill.ca/~hannah/MUMT621/gmm.pdf">http://www.music.mcgill.ca/~hannah/MUM…</a>&nbsp;<a href="#fnref:17" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:16">
<p>Han, B., Rho, S., Roger, Dannenberg, R. B. and Hwang, E. (2009) SMERS: Music Emotion Recognition using Support Vector Recognition [online] Available from:<a href="http://www.cs.cmu.edu/~rbd/papers/emotion-ismir-09.pdf"> http://www.cs.cmu.edu/~rbd/pap…</a>&nbsp;<a href="#fnref:16" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

</ol>
</div>
</div>
</body>
</html>
